#+TITLE: Abstractive Summarization and Extractive Generalization with NLP. 2022-10033-Standard Purchase Framework Agreement-Livedata Ltd-AI Researchers-P2021-065. Draft (2) report
#+AUTHOR: [[https://livedata.link][Livedata Limited]]

#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LaTeX_HEADER: \usepackage[margin=1in, headheight=40pt]{geometry}
#+LaTeX_HEADER: \let\description\compactdesc
#+LaTeX_HEADER: \usepackage{fancyhdr}
#+LaTeX_HEADER: \usepackage{lipsum}
#+LaTeX_HEADER: \pagestyle{fancy}
#+LaTeX_HEADER: \lhead{\includegraphics[width=20mm]{logo.png}}
#+LaTeX_HEADER: \chead{}
#+LaTeX_HEADER: \rhead{Livedata Limited}
#+LaTeX_HEADER: \usepackage{hyperref}
#+LaTeX_HEADER: \hypersetup{colorlinks,urlcolor=blue}

#+bibliography: bib.bib
#+cite_export: basic

* Intro
Currently, there is a wide variety of  Natural Language Processing (NLP) tasks; these range from a very basic pattern search to profoundly intelligent chat bots with the functionality to replace human operators.
In particular, there is a specific set of tasks which enables us to process an especially  large volume of papers - either in pdf or html format – in order to extract the most important information. Additionally,  it is sometimes necessary to remove certain information from the text to follow GDPR or other regulations.

* Problem definition
Thus, the primary aims of this project are:
    - Exploration of “abstractive” text summarisation and “extractive” text summarisation options, starting from the ground up and working towards the achievement of custom models - please find the term definition in the Glossary at section [[glos]].
    - Development of models during the exploration process (it is difficult to predict high accuracy).
    - Construction of prelimenary and very basic library/API prototype(s) where appropriate in Python and other programming languages.
    - Suitable test metric to assess the performance of the prototype solution (may not be possible for abstractive text summarisation).
    - Creation of a brief report with details regarding the methods explored, test results and best suggestions for further improvement. 

However, this project does NOT include the following:
    - A thorough technical review of all options. Instead, it will focus on allocating more time for the selection of promising options for detailed exploration and subsequent development.
    - Data collection[fn:1], since data has already been collected and provided.
    - Text insertion problem or “re-specification” of generalised text.

* Planning and exploration
** Key Stages
The project can be divided into the three stages described below. These stages correlate with the Order Form.
Currently, the first stage is fully completed, whilst the second and third stages are in progress.

- /Planning and meetings/ 
  - Meetings with the client and Luke Ellison (from Digital Catapult) to establish the initial scope of the project, gather information, deliver updates and present final reports. 
  - Individual planning time and creation of plan.
  - Mostly conducted in the last 2 weeks of May 2022, with some ongoing meetings for updates provision and further consultation when needed.

- /Individual Research/
  - A body of work to perform the required research. This includes background reading, data analysis, model creation, experimentation and analysis of results.
  - To be conducted in June, July and August 2022.

- /Report creation and work publication/
  - Production of a document outlining the results of the research collaboration and any work required in presenting or publishing these results.
  - A draft will be written prior to 1st August 2022, with the final report completed by mid-September.

** Methodology
The project is expected to be divided into the following stages:
    - obtaining the full training dataset from the client
    - defining the set of metrics to be used for an assessment of model quality
    - identifying the list of libraries or modules to be used for resolving abstractive summarisation and extractive generalisation tasks
    - training and assessing a set of distinct models, with special consideration given to the most appropriate metrics of performance
    - selecting one or more models that are best suited for resolving the necessary tasks
    - reaching a conclusion regarding package selection and subsequent model implementation for the NLP task at hand.

** Analysis result 
*** NLP models to be analysed
# Expand on this further
Ten of the most popular NLP packages are outlined in the article[cite:@art1]. At the time of writing, searching for “NLP” on [[https://pypi.org/search/?q=NLP][PyPI]] leads to more than 1,560 results, with similar results for [[https://libraries.io/search?languages=Python&q=NLP][Libraries.io]], given that 450 Python packages/libraries are found for the keyword.
The principal change which occured in the last few years is the more widespread adoption of neural networks, deep learning, and particularly Transformer-based models, such as [[https://arxiv.org/abs/1810.04805][BERT]] (late 2018), [[https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf][GPT-2]] (2019), and the relatively recent [[https://arxiv.org/abs/2005.14165][GPT-3]] (2020). Consequently, although ‘traditional’ approaches to NLP and computational linguistics are still widely utilised as viable solutions, the new models and methods are swiftly moving the entire field forward, enabling us to perform complex tasks via implementing the encoder-decoder architecture.

*** The most promising NLP models
The following criteria were employed to select the most promising libraries:
    - The library is Python-based - Python is the most popular language for all data science problems, including Natural Language Processing
    - The library is actively being developed and supported – Therefore, this ensures that the library remains reliable and suitable for professional usage in projects.
    - The library is multi-purpose – This enables us to utilise the library for various tasks, meaning that the need to switch between distinct libraries is eliminated.
    - The library possesses numerous GitHub stars - this reflects the popularity of a package and guarantees that the library is regularly tested by the GitHub community and likely contains few errors.

Based on this criteria, it is reasonable to conclude that the following ten packages are best suited for Natural Language Processing[fn:2][cite:@art2]:

- /[[https://github.com/huggingface/transformers][Transformers]]/: Provide easy access to a large number of powerful models (Transformer-based) that can be utilised directly or fine-tuned using PyTorch or TensorFlow.
- /[[https://github.com/explosion/spaCy][spaCy]]/: A powerful, all-purpose library with an intuitive API, an excellent ecosystem and sophisticated support for Deep Learning.
- /[[https://github.com/RaRe-Technologies/gensim][Gensim]]/: Excellent for highly-efficient and scalable topic and semantic modelling. Moreover, it provides easy access to Word2Vec and fastText.
- /[[https://github.com/nltk/nltk][NLTK]]/: The principal NLP library that has served as the basis for all other libraries. NLTK is still the de-facto standard for many NLP tasks and continues to be most effective for educational purposes.
- /[[https://github.com/flairNLP/flair][flair]]/: Very easy-to-use, whilst also providing powerful features, such as stacking embeddings and custom features such as “Flair embeddings.”
- /[[https://github.com/allenai/allennlp][AllenNLP]]/: Enables the construction of more sophisticated and accurate NLP models through   high-level abstractions. Furthermore, it has a very good CLI (Command-Line-Interface) for running pre-trained models.
- /[[https://github.com/sloria/TextBlob][TextBlob]]/: An ergonomic toolkit that is most effective at solving common, more ‘traditional’ NLP tasks.
- /[[https://github.com/stanfordnlp/stanza][Stanza]]/: Provides a complete and robust NLP pipeline, whilst also serving as an efficient all-purpose library. It is language-agnostic and currently supports 66 languages with pre-trained neural models; this includes access to Stanford’s CoreNLP.
- /[[https://github.com/NervanaSystems/nlp-architect][NLP Architect]]/: Allows for exploration and experimentation with various state-of-the-art techniques.
- /[[https://github.com/JohnSnowLabs/spark-nlp][Spark NLP]]/: Enables us to run complex NLP pipelines in a distributed environment and contains over 220 pre-trained pipelines and models that can be easily utilised for resolving various tasks.

*** Metrics
We proceeded to conduct a number of tests to gauge the effectiveness of libraries from the list above, in order to find two of the most applicable libraries for reaching the goals described above. Flexibility and transformation accuracy were considered to be the most criteria during the testing process.

Therefore, a specific set of metrics was identified to confirm or disprove the applicability of the model and library for completing the task at hand.

There are both standard and specific quality metrics[cite:@art3] that can be used to assess model quality.
Standard metrics:
    - Accuracy and precision
    - Recall 
    - F1-score 
    - Area Under Curve
    - Mean Reciprocal Rank and others
NLP-specific metrics:
    - BLEU[cite:@howtoBert] (BiLingual Evaluation Understudy, using the set of available libraries) - notebook with examples can be downloaded from GitHub [[https://github.com/gcunhase/NLPMetrics/blob/master/notebooks/bleu.ipynb][here]]
    - GLEU (Google-BLEU)
    - BERTSCORE (compute token similarity using contextual embeddings) - the example of code can be found [[https://github.com/Tiiiger/bert_score][here]]
    - WER (Word Error Rate) and others

However, the more task specific metrics[cite:@spec] have been identified as the following:

Abstractive summarization[cite:@abs_sum]:
    - [[https://spacy.io/api/span/#similarity][Similarity]] (as a part of SpaCy)
    - BERTSCORE
    - ROUGE (Recall-Oriented Understudy for Gisting Evaluation) – able to utilise any Python library[cite:@rouge]; the python library to calculate such a metric is available [[https://pypi.org/project/rouge/][here]]. 

Extractive generalization:
    - Similarity (SpaCy)
    - BLEU
    - BERTSCORE

The customer provided us with a training dataset for both abstractive summarization and extractive generalisation. After a comprehensive set of experiments, we concluded that the usage of a cosine-based similarity criteria was most efficient for assessing the quality of the model. This result can be explained due to other criteria - such as BLEU or BERTScore – being either more oriented towards assessing translation tasks (assessing the number of the same or similar words when comparing results to expectations) or towards using the cosine similarity approach (based on word vectors in the 300-D array of data). Furthermore, for tasks such as abstractive summarization – which are more related to object similarity than counting identical words - we decided to use similarity for both targets. This can be explained by the fact that in abstractive summarization, usually a new set of both words and sentences is created, with changes to the sentence structure being an ordinary occurrence. Thus, the use of direct sentence comparison techniques – such as BLEU – is not appropriate here.

Additionally, given that we closely follow the Agile methodology, we have developed a simple API – which is available on GitHub - to assess the training dataset and experiment with any other texts. This is profoundly important because this adds additional flexibility to our model, allowing for further usage in other applications without significant adaptations.

*** Methodology to be used
Let us recall the highest priority task for the project: 
      “	Exploration of ‘abstractive’ text summarisation and ‘extractive’ text summarisation options. Starting from the ground up and working towards custom models.”[cite:@abs_sum_2]

To resolve the summarisation task two approaches can be used:
    - There are numerous models to conduct extractive summarisation. Extractive summarisation is usually based on an assessment of ‘sentence importance’, which is realised into python library [[https://pypi.org/project/pysummarization/][pysummarization]].
      # The phrase above can be moved into intro
       The first approach entails the selection of the most ‘important’ sentences through a word frequency analysis; the system subsequently selects and copies a selection of sentences from the source without any changes to the sentences. The benefits of using this approach are: 
      (a) the sentences usually make more sense in comparison to a machine-generated alternatives, since they come from the original text and likely do not require additional alignment.
      (b) the method is sufficiently simple and swift in realising the solution – it enables the usage of very basic NLP packages such as NLTK or even RegExpr to fully resolve the problem.
      
    - Another approach is based on Deep Neural Networks (DNNs). The architecture of DNNs is derived from that of the human brain, whereby layers of neurons are intertwined with each other through different ‘weights’ for each connection (i.e. the process of training the network). The weight coefficients are chosen in accordance to specific mathematical functions, so that each input has a unique, desirable output to match the expected result. DNNs are primarily utilised when it is substantially difficult to adhere to a large number of rules and conditions, since DNNs are extremely flexible and contain numerous parameters.
 
    - The most important neural net architectures[cite:@ml2] are feed-forward neural nets for tabular data, convolutional nets for computer vision, recurrent nets and long short-term memory (LSTM) nets for sequence processing, encoder/decoders and Transformers for Natural Language Processing, as well as auto-encoders and Generative Adversarial Networks (GANs) for generative learning.
      
      These approaches allow us to produce a new and completely unique text to summarise the source, and it is hence labelled as abstractive summarisation. To determine the accuracy level, we tested all of the packages described above and came to the conclusion that the best results are presented using the [[https://github.com/huggingface/transformers][Transformers]] package (through the default BERT[cite:@bert] library) and the PEGASUS6 library. The first package shows more flexibility, since it allows us to select the most effective language module for further utilisation and has a less significant size (approximately 1GB less) in comparison to PEGASUS by default. Thus, it is much swifter in operation and contains more sophisticated performance metrics. 

Extractive generalisation demands the removal of any [company] specific information from the source. Our findings have shown that such tasks can be resolved using a Named Entity Recognition (NER) approach, which is usually included in many NLP packages by default. After a set of comprehensive tests, we discovered that the best results were shown when using the SpaCy1 library, as it contains a number of pre-trained language models which consist of numerous embedded functionalities, including tokenization, lemmatization, Point-Of-Speech (POS) extraction and named-entity extraction. In our [[https://github.com/turbaevsky/DigiCatapult-summ][API at GitHub]] - which is deployed at https://livedata.link/nlp/upload - we had created an NER-analyser and subsequently removed all of the sentences that contain any organisational or personal names, dates, currencies, geographical locations and other sensitive data. As the task requested a completion of extractive summarisation, this approach has been determined to be the most reasonable, because it provides the highest similarity metric in comparison to other potential approaches.

* Model Usage Results
Despite the fact that the most comprehensive models - such as BERT, GPT(s) or PEGASUS - can provide the required functionality, our aim is to provide the most efficient service for the tasks at hand. Following the comprehensive testing process, it can be confirmed that the following hypotheses are correct:
    - SpaCy[cite:@spacy] has an advanced linguistic module that is capable of undertaking all of the necessary linguistic transformations and analysis. Additionally, it contains a list of modules to be used for heavily individual tasks, such as semantic analysis and entity extraction. This includes the ability to connect any third-party Neural Networks framework, such as TensorFlow or PyTorch. Notably, even though there is extensive functionality provision, SpaCy is relatively more overwhelming in comparison to other libraries.
    - Transformers is a framework that is utilised to adapt any third-party libraries, including TensorFlow and BERT. Thus, Transformers allows us to efficiently employ all of the privileges of transforming learning, although it is important to note that sometimes this makes the resulting library overweight or too complex for commercial service use.

** Metrics comparison
*** Abstractive summarisation
Below, you can see the result of BERT-based /transformers/ and /SpaCy-based/ absractive summarisation models.

#+BEGIN_center
#+ATTR_LaTeX: :height 0.3\textwidth :center
#+NAME:   fig:bert-abs
[[./sim_trans.png]]
#+ATTR_LaTeX: :height 0.3\textwidth :center
#+name: fig:spacy-ext
[[./sim_tf_idf.png]]
#+END_center

Surprisingly, it became clear that the extractive summarisation model showed a better similarity metric than the more advanced BERT model.

*** Extractive generalisation
To resolve the extractive generalisation task, we assumed that /point-of-speech/ (POS) and entity-type recognition should be used to optimise performance, whereby all the sentences containing either geographical position, name, or numbers are removed. Notably, the deletion of any organisation names would also be significant in enhancing performance.

To assess the quality of the model the following approach has been utilised:
- the source dataset has been converted into a new one by creating a new column which contains the sentences /to be removed/ from the source text.
- the SpaCy-based POS and entity type recognition were used for selecting sentences which have any /specific/ information, such as location, name of business, etc.
- the /corpus-based BLEU/ metric[fn:4] was used to compare sentences that have been removed with the sentences which are /expected/ to be removed.

As a result of the comparison, the following chart was constructed.

#+name: fig:bleu
#+caption: BLEU score for extractive generalisation test
[[./bleu.png]]

The comparison's result is not ideal. It means that additional efforts must be used to improve the quality of the model, or different approaches should be selected and tested to resolve the issue. Also, some extra clarification or another training dataset may be required.


** Conclusion

Therefore, following the successful completion of our experiments, we are able to conclude that:

- the most applicable library to be used for /abstractive summarisation/ is the ‘summarisation’ module, based on a BERT library, such as /transformers/.
  
However, the similarity metric which was used for the training dataset demonstrated that simple 'TF-IDF'-based summarisation works faster than BERT-based abstractive summarisation, displaying higher similarity. The result of the processing is presented above.

- the PEGASUS module has also provided a good result (as shown below), but it requires more resources to ensure effective operation. Hence, it occupies the second spot of our rating.

- at the same time, to complete an /extractive generalisation/ task, the SpaCy module has proved to be one of the most effective, as confirmed by the image [[fig:bleu]] above.

* Bibliography
# HowTo:
# https://orgmode.org/manual/Citation-handling.html
# https://github.com/jkitchin/org-ref/blob/master/org-ref.org
# https://orgmode.org/manual/Citation-export-processors.html
# https://blog.tecosaur.com/tmio/2021-07-31-citations.html

#+print_bibliography:

#+LaTeX: \appendix
* Glossary <<glos>>
- /Abstractive summarization/: Abstractive Text Summarization is the process of generating a short and conci@ml2ummary that captures the salient ideas of the source text[cite:@gl1]. The generated summaries may potentially contain new phrases and sentences that may not appear in the source text.
- /Extractive generalization/: In our context, extractive generalization reflects the removal of any entity-related specific information, such as an organization’s name, dates, currency value, etc. The extraction process may either copy non-specific sentences or create a summary based on the key information present.
- /Abstractive[fn:3]/: Rephrasing a block of text.
- /Extractive/: Extracting data from a block of text only

* Source code and results
** API
#+include: "~/projects/10be5/code/api.py" src python
** BLEU
#+include: "~/projects/10be5/code/bleu.py" src python
** PEGASUS
#+include: "~/projects/10be5/code/pegasus.py" src python
** SpaCy TF-IDF based summarisation
#+include: "~/projects/10be5/code/spacy_tst.py" src python
** Transformers-based abstractive summarisation
#+include: "~/projects/10be5/code/summarization.py" src python

* Training dataset examples
#+include: "~/projects/10be5/data/example.csv" src text

* Footnotes
[fn:1] The training dataset is available online and sample data is presented in the attachment.
[fn:2] The ‘GitHub stars criteria’ was used for sorting purposes
[fn:3] Extractive summary is choosing specific sentences from the text to compile a summary, while abstractive summary means generating a summary in the computer’s own words.
[fn:4] Corpus score calculation compares 1 candidate document with multiple sentence and 1+ reference documents also with multiple sentences. It is differ from averaging BLEU scores of each sentence, it calculates the score by /summing the numerators and denominators for each hypothesis-reference(s) pairs before the division/
